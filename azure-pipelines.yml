name: GBlog-$(SourceBranchName)-$(Date:yyyyMMdd)-$(Rev:.r)

trigger:
- master

pr: none

pool:
  vmImage: ubuntu-latest

variables:
  artifact_name: website

jobs:
- job: Build
  displayName: Download and run Hugo
  workspace:
    clean: all
  timeoutInMinutes: 2
  variables:
    hugo_download_url: https://github.com/gohugoio/hugo/releases/download/v0.59.1/hugo_extended_0.59.1_Linux-64bit.tar.gz
    hugo_archive: hugo.tar.gz
    hugo_build_path: $(System.DefaultWorkingDirectory)/$(artifact_name)
  steps:
  - bash: |
      wget --output-document $(hugo_archive) --tries 3 --no-verbose $(hugo_download_url)
      tar -xzvf $(hugo_archive) hugo
      rm $(hugo_archive)
      ./hugo version
  - bash: |
      ./hugo --destination $(hugo_build_path) --cleanDestinationDir --gc --ignoreCache --log --minify --noChmod
      echo "Files and folders generated by Hugo:"
      ls $(hugo_build_path)
  - publish: $(hugo_build_path)
    artifact: $(artifact_name)

- deployment: Deploy
  displayName: Deploy to server
  dependsOn: Build
  environment: GBlog-prod
  workspace:
    clean: all
  timeoutInMinutes: 2
  variables:
    artifact_path: $(Pipeline.Workspace)/$(artifact_name)
  strategy:
    runOnce:
      deploy:
        steps:
        # Specify download step in order to only download website artifact
        - download: current
          artifact: $(artifact_name)
        - bash: ls $(artifact_path)
        - task: FtpUpload@2
          inputs:
            credentialsOption: inputs
            serverUrl: $(ftp_server)
            username: $(ftp_user)
            password: $(ftp_password)
            rootDirectory: $(artifact_path)
            filePatterns: '**'
            remoteDirectory: /
            clean: true
            preservePaths: true
            trustSSL: true